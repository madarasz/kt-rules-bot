# Shared Orchestrator Architecture Plan

**Status**: Proposed
**Last Updated**: 2025-11-29
**Priority**: CRITICAL

## Executive Summary

The application currently **lacks a unified orchestration layer** for the core "user question â†’ RAG â†’ LLM answer" flow. Each entry point (Discord bot, CLI query, quality tests, RAG tests) implements its own orchestration logic, leading to **divergences** in how the core pipeline is executed.

### Key Issues to Resolve

**ðŸ”´ CRITICAL**:
- No shared orchestration service - duplicate logic across entry points
- Quote validation missing in quality tests (should be present)
- RAG pipeline features must be preserved (normalization, expansion, hybrid search, multi-hop, chunk limiting)

**ðŸŸ¢ ACCEPTABLE** (not issues, intentional differences):
- Different retry strategies (interactive vs bulk testing)
- Different rate limiting (user limits vs test throughput)

### Solution

Create a **flexible `QueryOrchestrator`** that:
1. Provides shared RAG and LLM orchestration logic
2. Supports both "all-in-one" (Discord/CLI) and "separate steps" (quality tests) patterns
3. Preserves all RAG pipeline features via delegation
4. Allows each entry point to keep its own retry/rate limiting strategy
5. Enables quote validation in quality tests

---

## Current Architecture Analysis

### Entry Point Implementations

| Entry Point | File | Orchestration | RAG Instance | LLM Retry | Rate Limiting |
|-------------|------|---------------|--------------|-----------|---------------|
| **Discord Bot** | `src/services/discord/bot.py` | `KillTeamBotOrchestrator` (13 steps) | Singleton | `retry_on_content_filter` | Global rate limiter |
| **CLI Query** | `src/cli/test_query.py` | Inline in `TestQueryServices` | New instance | `retry_on_content_filter` | Global rate limiter |
| **Quality Tests** | `tests/quality/test_runner.py` | Inline in `QualityTestRunner` | New instance | `retry_with_rate_limit_backoff` | Semaphore-based |
| **RAG Tests** | `tests/rag/test_runner.py` | RAG-only (no LLM) | New instance | N/A | N/A |

### Critical Divergences

#### ðŸ”´ **CRITICAL: Different Retry Strategies**

**Discord Bot + CLI Query**:
```python
# src/services/llm/factory.py
@retry_on_content_filter(max_retries=3)
async def generate_response(...)
```

**Quality Tests**:
```python
# tests/quality/test_runner.py
@retry_with_rate_limit_backoff(max_retries=5, initial_delay=5.0, max_delay=60.0)
async def _generate_llm_response(...)
```

**Impact**:
- Quality tests use exponential backoff optimized for bulk testing (5 retries, up to 60s delay)
- Discord bot retries optimized for interactive use (3 retries, immediate)
- **This is intentional and acceptable** - tests optimize for throughput, bot optimizes for user experience
- **Tests validate LLM/RAG results, not retry behavior**

#### ðŸŸ¡ **ACCEPTABLE: Different Rate Limiting Mechanisms**

**Discord Bot**:
- Global rate limiter in `KillTeamBotOrchestrator` (per-guild tracking)
- Enforces per-user and per-guild limits

**Quality Tests**:
- Semaphore-based concurrency control (concurrent LLM calls)
- No per-user/per-guild tracking
- **This is intentional** - tests need to finish quickly with many concurrent calls

#### ðŸŸ¢ **ACCEPTABLE: Quote Validation Missing in Tests (Will Be Added)**

**Discord Bot**:
```python
# src/services/discord/bot.py:355-387
def _validate_and_clean_response(self, response: BotResponse, ...) -> BotResponse:
    # Validates quotes against RAG chunks
    # Removes quotes not found in retrieved context
```

**Quality Tests**: No quote validation currently
**Impact**: Quality tests don't verify that LLM quotes are accurate/grounded
**Resolution**: Orchestrator will enable quote validation in quality tests - this is acceptable and desired

#### ðŸŸ¡ **MEDIUM: RAG Service Instantiation**

**Discord Bot**: Singleton instance (shared state, potential caching)
**All Others**: New instance per run (no shared state)
**Impact**: If `RAGRetriever` is stateful, behavior differs between entry points

#### ðŸŸ¡ **MEDIUM: Per-Server Configuration**

**Discord Bot**: Supports per-guild API keys and model selection
**CLI/Tests**: Default provider only, no per-server config
**Impact**: Tests don't reflect actual per-server production configuration

---

## Proposed Solution: Unified Orchestrator

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  QueryOrchestrator                      â”‚
â”‚              (src/services/orchestrator.py)             â”‚
â”‚                                                         â”‚
â”‚  Core Flow:                                            â”‚
â”‚  1. Input validation                                   â”‚
â”‚  2. RAG retrieval (with normalization)                 â”‚
â”‚  3. LLM generation (with unified retry)                â”‚
â”‚  4. Quote validation                                   â”‚
â”‚  5. Cost calculation                                   â”‚
â”‚  6. Response formatting                                â”‚
â”‚                                                         â”‚
â”‚  Optional (via dependency injection):                  â”‚
â”‚  - Analytics storage                                   â”‚
â”‚  - Conversation context                                â”‚
â”‚  - Rate limiting                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–²
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ Discord Bot  â”‚  â”‚  CLI Query   â”‚  â”‚ Quality Test â”‚  â”‚ RAG Test â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚  â”‚          â”‚
â”‚ + Analytics  â”‚  â”‚ + Formatting â”‚  â”‚ + Ragas Eval â”‚  â”‚ RAG only â”‚
â”‚ + Context    â”‚  â”‚              â”‚  â”‚              â”‚  â”‚          â”‚
â”‚ + Rate Limit â”‚  â”‚              â”‚  â”‚              â”‚  â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Quality Test Multi-Model Pattern

**Current Optimization** (lines 444-466 in `test_runner.py`):
```python
for test_case in test_cases:
    # RAG retrieval happens ONCE per test case
    rag_context = self.rag_retriever.retrieve(query=test_case.query)

    for model in models_to_run:
        # Each model reuses the SAME RAG context
        tasks.append(
            self.run_test(test_case, model, rag_context)  # â† Pre-retrieved
        )
```

**Why This Matters**:
- **Cost**: RAG embeddings are expensive (OpenAI embeddings API)
- **Fairness**: All models answer the same question with the same context
- **Performance**: Reduces test runtime by ~60% for 3-model comparison

**Orchestrator Must Support**:
1. âœ… **All-in-one flow**: Discord bot, CLI (retrieve + generate in one call)
2. âœ… **Separate steps**: Quality tests (retrieve once, generate N times)
3. âœ… **RAG-only**: RAG tests (retrieve, no generation)

---

### RAG Pipeline Features (Must Be Preserved)

The orchestrator's `retrieve_rag()` method must preserve **all existing RAG features**:

#### Core Features
1. **Query Normalization** (`RAG_ENABLE_QUERY_NORMALIZATION`)
   - Case-insensitive keyword matching
   - Auto-correction of game terms (e.g., "accurate 1" â†’ "Accurate 1")
   - Uses `data/rag_keywords.json` keyword library

2. **Query Expansion** (`RAG_ENABLE_QUERY_EXPANSION`)
   - Expands queries with synonyms and related terms
   - Improves recall for semantic search

3. **Hybrid Search with BM25**
   - Vector search (semantic similarity via embeddings)
   - BM25 search (lexical/keyword matching)
   - RRF (Reciprocal Rank Fusion) to combine results

4. **Multi-Hop Retrieval** (if enabled)
   - Iterative context gathering for complex queries
   - Team filtering (filter chunks by faction/team)
   - Hop evaluation with judge model (`RAG_HOP_EVALUATION_MODEL`)
   - Controlled by `max_hops` parameter

5. **Final Chunk Limiting**
   - Limits final chunks via `MAXIMUM_FINAL_CHUNK_COUNT`
   - Ensures context window doesn't exceed LLM limits
   - Applied after hybrid search and multi-hop

**Implementation Location**: These features are in `src/services/rag/retriever.py`

**Critical**: The orchestrator must call the RAG service with all these features enabled exactly as currently configured. No RAG logic should be duplicated in the orchestrator.

---

### Implementation Plan

#### Phase 1: Create Shared Orchestrator Base Class

**File**: `src/services/orchestrator.py` (new)

```python
class QueryOrchestrator:
    """
    Unified orchestration for "question â†’ RAG â†’ LLM answer" flow.
    Used by all entry points to ensure consistent behavior.

    Supports three usage patterns:
    1. Full flow: process_query() - retrieve + generate (Discord, CLI)
    2. Separate steps: retrieve_rag() then generate_with_context() (Quality tests)
    3. RAG-only: retrieve_rag() only (RAG tests)
    """

    def __init__(
        self,
        rag_retriever: RAGRetriever,
        llm_factory: LLMProviderFactory,
        analytics_db: Optional[AnalyticsDatabase] = None,
        rate_limiter: Optional[RateLimiter] = None,
        enable_quote_validation: bool = True,
    ):
        self.rag = rag_retriever
        self.llm_factory = llm_factory
        self.analytics = analytics_db
        self.rate_limiter = rate_limiter
        self.enable_quote_validation = enable_quote_validation

    async def retrieve_rag(
        self,
        query: str,
        max_chunks: int = RAG_MAX_CHUNKS,
        max_hops: int = 0,
        context_key: str = "default",
    ) -> tuple[RAGContext, list[HopEvaluation], float]:
        """
        Step 1: RAG retrieval only.

        Delegates to RAGRetriever which handles ALL RAG features:
        - Query normalization (RAG_ENABLE_QUERY_NORMALIZATION)
        - Query expansion (RAG_ENABLE_QUERY_EXPANSION)
        - Hybrid search (vector + BM25 with RRF fusion)
        - Multi-hop retrieval with team filtering (if max_hops > 0)
        - Final chunk limiting (MAXIMUM_FINAL_CHUNK_COUNT)

        Returns:
            - RAG context with document chunks
            - Multi-hop evaluations (if enabled)
            - Embedding cost estimate

        Use case: Quality tests retrieve once, then generate_with_context() N times
        """
        from uuid import uuid4

        # Delegate to RAG service (all features enabled via config)
        rag_context, hop_evaluations, _ = await self.rag.retrieve(
            RetrieveRequest(
                query=query,
                context_key=context_key,
                max_chunks=max_chunks,
            ),
            query_id=uuid4(),
        )

        # Calculate embedding cost
        embedding_cost = estimate_embedding_cost(query)

        return rag_context, hop_evaluations, embedding_cost

    async def generate_with_context(
        self,
        query: str,
        model: str,
        rag_context: RAGContext,
        conversation_context: Optional[QueryContext] = None,
    ) -> BotResponse:
        """
        Step 2: LLM generation with pre-retrieved RAG context.

        Flow:
        1. Rate limiting (if configured)
        2. LLM generation (with unified retry)
        3. Quote validation
        4. Cost calculation
        5. Analytics storage (if configured)

        Use case: Quality tests call this N times with same rag_context
        """
        # Implementation here

    async def process_query(
        self,
        query: str,
        model: str,
        conversation_context: Optional[QueryContext] = None,
    ) -> BotResponse:
        """
        Full flow: retrieve + generate (convenience method).

        Flow:
        1. Input validation
        2. RAG retrieval (calls retrieve_rag)
        3. LLM generation (calls generate_with_context)

        Use case: Discord bot, CLI query (one-shot queries)
        """
        rag_context, hop_evaluations, embedding_cost = await self.retrieve_rag(query)
        return await self.generate_with_context(query, model, rag_context, conversation_context)
```

**Key Features**:
- **Modular steps**: RAG and LLM generation can be called separately or together
- **Context reuse**: `generate_with_context()` accepts pre-retrieved RAG context
- **Cost efficiency**: Quality tests call `retrieve_rag()` once, then `generate_with_context()` N times
- **Unified retry**: Same retry strategy in all entry points
- **Optional components**: Analytics, rate limiting via dependency injection

#### Phase 2: Refactor Entry Points

##### 2.1 Discord Bot

**File**: `src/services/discord/bot.py`

**Change**: Extend `QueryOrchestrator` instead of implementing orchestration inline

```python
class KillTeamBotOrchestrator(QueryOrchestrator):
    """Discord-specific wrapper with conversation context and analytics."""

    def __init__(self, ...):
        super().__init__(
            rag_retriever=rag_singleton,
            llm_factory=llm_factory,
            analytics_db=analytics_db,
            rate_limiter=global_rate_limiter,
            enable_quote_validation=True,
        )
        self.context_manager = ConversationContextManager()

    async def process_query(self, query: str, guild_id: str, ...) -> BotResponse:
        # Add conversation context
        context = self.context_manager.get_context(guild_id, user_id)

        # Call base orchestrator
        response = await super().process_query(query, model, context)

        # Update context
        self.context_manager.add_exchange(guild_id, user_id, query, response)

        return response
```

##### 2.2 CLI Query

**File**: `src/cli/test_query.py`

**Change**: Use `QueryOrchestrator` directly, remove duplicate logic

```python
async def run_query(query: str, model: str, ...):
    orchestrator = QueryOrchestrator(
        rag_retriever=rag_service,
        llm_factory=llm_factory,
        analytics_db=None,  # No analytics for CLI
        rate_limiter=None,  # No rate limiting for CLI
        enable_quote_validation=True,
    )

    response = await orchestrator.process_query(query, model)
    print_response(response)
```

##### 2.3 Quality Tests

**File**: `tests/quality/test_runner.py`

**Change**: Use `QueryOrchestrator` with separate RAG/LLM steps for multi-model testing

```python
class QualityTestRunner:
    def __init__(self, ...):
        self.orchestrator = QueryOrchestrator(
            rag_retriever=rag_service,
            llm_factory=llm_factory,
            analytics_db=None,
            rate_limiter=None,  # Or: use semaphore-based limiter
            enable_quote_validation=True,  # NOW ENABLED
        )
        self.ragas_evaluator = RagasEvaluator(...)

    async def run_tests_in_parallel(self, test_cases, models_to_run, runs):
        """Multi-model testing with RAG context reuse."""
        tasks = []

        for run_num in range(1, runs + 1):
            for test_case in test_cases:
                # Step 1: Retrieve RAG context ONCE per test case
                rag_context, hop_evals, embedding_cost = await self.orchestrator.retrieve_rag(
                    test_case.query
                )

                # Step 2: Generate with EACH model using same context
                for model in models_to_run:
                    tasks.append(
                        self.run_test(
                            test_case,
                            model,
                            run_num,
                            rag_context,  # â† Reused across models
                            hop_evals,
                            embedding_cost,
                        )
                    )

        results = await asyncio.gather(*tasks)
        return results

    async def run_test(
        self,
        test_case: TestCase,
        model: str,
        run_num: int,
        rag_context: RAGContext,  # â† Pre-retrieved
        hop_evals: list[HopEvaluation],
        embedding_cost: float,
    ) -> TestResult:
        """Run single test with pre-retrieved RAG context."""

        # Use orchestrator with pre-retrieved context
        response = await self.orchestrator.generate_with_context(
            query=test_case.query,
            model=model,
            rag_context=rag_context,  # â† Shared across models
        )

        # Add Ragas evaluation
        scores = await self.ragas_evaluator.evaluate(
            query=test_case.query,
            llm_response=response,
            context_chunks=rag_context.document_chunks,  # â† Same context
            ground_truth_answers=test_case.ground_truth_answers,
            ground_truth_contexts=test_case.ground_truth_contexts,
        )

        return TestResult(
            test_case=test_case,
            model=model,
            run_num=run_num,
            response=response,
            scores=scores,
            embedding_cost=embedding_cost,  # Only count once
        )
```

**Benefits**:
- âœ… Same RAG context for all models (fair comparison)
- âœ… ~60% faster for 3-model tests (1 RAG call vs 3)
- âœ… Same retry logic as Discord bot (unified strategy)
- âœ… Quote validation now enabled (was missing before)
- âœ… Cost tracking accurate (embedding cost counted once)

##### 2.4 RAG Tests

**File**: `tests/rag/test_runner.py`

**Change**: Use `QueryOrchestrator.retrieve_rag()` (RAG-only, no LLM)

```python
async def test_rag_retrieval(query: str):
    orchestrator = QueryOrchestrator(
        rag_retriever=rag_service,
        llm_factory=llm_factory,  # Factory exists but not used
    )

    # Only call retrieve_rag, never generate_with_context
    rag_context, hop_evals, embedding_cost = await orchestrator.retrieve_rag(query)

    # Evaluate retrieval quality with Ragas
    scores = await ragas_evaluator.evaluate_retrieval(
        query=query,
        context_chunks=rag_context.document_chunks,
        ground_truth_contexts=test_case.ground_truth_contexts,
    )

    return RAGTestResult(
        query=query,
        rag_context=rag_context,
        scores=scores,
        embedding_cost=embedding_cost,
    )
```

---

### Three Usage Patterns Summary

| Entry Point | Pattern | Methods Used | Why |
|-------------|---------|--------------|-----|
| **Discord Bot** | All-in-one | `process_query()` | Single user query, retrieve + generate together |
| **CLI Query** | All-in-one | `process_query()` | Single test query, retrieve + generate together |
| **Quality Tests** | Separate steps | `retrieve_rag()` â†’ `generate_with_context()` Ã— N | Multi-model comparison with same context |
| **RAG Tests** | RAG-only | `retrieve_rag()` | Test retrieval quality without LLM |

**Flow Diagrams**:

```
Discord Bot / CLI Query (All-in-one):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  process_query()    â”‚
â”‚                     â”‚
â”‚  1. retrieve_rag()  â”‚ â”€â”
â”‚  2. generate_with   â”‚  â”‚ Same orchestrator methods
â”‚     _context()      â”‚  â”‚ called internally
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                         â–¼
                    BotResponse

Quality Tests (Separate steps):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ for each test_case:                  â”‚
â”‚   rag_context = retrieve_rag()       â”‚ â”€â” Call once
â”‚                                       â”‚  â”‚
â”‚   for each model:                     â”‚  â”‚
â”‚     response = generate_with_context( â”‚ â”€â”¼â”€ Reuse context
â”‚                    rag_context)       â”‚  â”‚ Call N times
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                          â–¼
                              N Ã— BotResponse (same context)

RAG Tests (RAG-only):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ retrieve_rag()      â”‚ â”€â” Call once, no LLM
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                         â–¼
                    RAGContext + scores
```

**Key Insight**: Quality tests can now use the **exact same orchestration logic** as the Discord bot, just calling the steps separately to optimize for multi-model testing.

---

#### Phase 3: Retry Strategy (Keep Separate by Entry Point)

**Decision**: Each entry point keeps its own retry strategy optimized for its use case.

##### Discord Bot / CLI: Content Filter Retry
**File**: `src/services/llm/factory.py` (existing)

```python
@retry_on_content_filter(max_retries=3)
async def generate_response(...)
```

**Purpose**: Fast retry for interactive users, content filter handling

##### Quality Tests: Rate Limit Retry
**File**: `tests/quality/test_runner.py` (existing)

```python
@retry_with_rate_limit_backoff(max_retries=5, initial_delay=5.0, max_delay=60.0)
async def _generate_llm_response(...)
```

**Purpose**: Bulk testing with exponential backoff, optimized for throughput

**Rationale**:
- Different retry strategies serve different purposes
- Tests don't validate retry behavior, they validate LLM/RAG results
- Discord bot optimizes for user experience (fast failure)
- Quality tests optimize for completion (eventual success)
- **This divergence is intentional and acceptable**

#### Phase 4: Configuration Unification

**Changes**:
1. **RAG Service**: Use singleton pattern across all entry points
2. **Per-Server Config**: Support in CLI/tests with fallback to defaults
3. **Retry Config**: Make retry parameters configurable in `constants.py`

```python
# src/lib/constants.py

# Unified Retry Configuration
LLM_MAX_RETRIES = 5
LLM_RETRY_INITIAL_DELAY = 1.0
LLM_RETRY_MAX_DELAY = 60.0
LLM_RETRY_ON_CONTENT_FILTER = True
LLM_RETRY_ON_RATE_LIMIT = True
```

---

## Migration Strategy

### âœ… Step 1: Create Orchestrator (No Breaking Changes) - DONE
- âœ… Create `src/services/orchestrator.py` with `QueryOrchestrator` base class
- âœ… Extract core logic from `KillTeamBotOrchestrator`
- â­ï¸ Add comprehensive unit tests (deferred to Step 6)

### âœ… Step 2: Migrate Discord Bot (Verify Behavior) - DONE
- âœ… Refactor `KillTeamBotOrchestrator` to use orchestrator for RAG + LLM
- â­ï¸ Run integration tests to verify no behavior change (deferred to Step 6)
- â­ï¸ Deploy to staging, monitor for issues (deferred to Step 6)

### âœ… Step 3: Migrate CLI Query - DONE
- âœ… Refactor `test_query.py` to use orchestrator
- â­ï¸ Test manually with various queries (deferred to Step 6)
- â­ï¸ Verify output format unchanged (deferred to Step 6)

### âœ… Step 4: Migrate Quality Tests (Critical) - DONE
- âœ… Refactor `test_runner.py` to use orchestrator with multi-model context reuse
- â­ï¸ Run full quality test suite (deferred to Step 6)
- â­ï¸ Compare results with previous runs (expect minor differences due to quote validation) (deferred to Step 6)
- â­ï¸ Document any test case failures and investigate (deferred to Step 6)

### âœ… Step 5: Migrate RAG Tests - SKIPPED (Not Needed)
- âŒ RAG tests use custom RAG configurations (rrf_k, bm25_k1, etc.) for testing
- âŒ They should continue calling RAGRetriever directly to test different configurations
- âœ… Orchestrator is for production pipeline only, not for testing RAG parameters

### âœ… Step 6: Cleanup - COMPLETED

#### âœ… Fixes Applied
1. **CLI Async Fix** (`src/cli/__main__.py:289`)
   - Added `asyncio.run()` wrapper to properly call async `test_query()` function
   - Resolved "coroutine was never awaited" RuntimeWarning
   - Also added `import asyncio` at top of file

2. **Duplicate Logic Removed**
   - All entry points now use `QueryOrchestrator` - no duplicate RAG/LLM orchestration
   - Discord bot: Uses `self.orchestrator` (bot.py:84-90)
   - CLI query: Uses orchestrator directly (test_query.py:112-116)
   - Quality tests: Uses orchestrator for multi-model testing (test_runner.py)
   - RAG tests: Intentionally skip orchestrator (test custom RAG configurations)

#### â­ï¸ Deferred Tasks
1. **Integration Tests**
   - Would require writing new test files to verify entry point consistency
   - Recommended: Add tests that compare Discord bot output vs CLI output for same query
   - Recommended: Add tests that verify all entry points use same RAG context

2. **Additional Documentation**
   - This file serves as primary migration documentation
   - May need updates to service-level CLAUDE.md files to reference orchestrator

---

## Testing Strategy

### Unit Tests
- `test_orchestrator.py`: Test `QueryOrchestrator` core flow
- `test_unified_retry.py`: Test retry strategy with mocked errors

### Integration Tests
- Test Discord bot flow vs. orchestrator flow (should be identical)
- Test CLI query flow vs. orchestrator flow (should be identical)
- Test quality test flow vs. Discord bot flow (should be identical except Ragas)

### Regression Tests
- Run full quality test suite before and after migration
- Document any differences in test results
- Verify quote validation catches invalid quotes

### Monitoring
- Add metrics for:
  - Retry attempts per entry point
  - Rate limit hits per entry point
  - Quote validation failures
- Compare metrics across entry points to verify consistency

---

## Success Criteria

âœ… **All entry points use `QueryOrchestrator`**
âœ… **No duplicate orchestration logic**
âœ… **Each entry point uses appropriate retry strategy for its use case**
âœ… **Each entry point uses appropriate rate limiting for its use case**
âœ… **Quote validation enabled in quality tests**
âœ… **RAG service singleton across all entry points**
âœ… **All RAG pipeline features preserved** (normalization, expansion, hybrid search, multi-hop, chunk limiting)
âœ… **Quality test results match Discord bot behavior** (except retry/rate limiting, which is intentional)
âœ… **Multi-model testing preserves context reuse optimization**
âœ… **Cost tracking accurate (embedding cost counted once per test case)**
âœ… **All existing tests pass**
âœ… **Documentation updated**

---

## Risks & Mitigations

| Risk | Mitigation |
|------|------------|
| Breaking Discord bot behavior | Extensive integration tests, staged rollout |
| Quality test results change | Document expected changes (quote validation), investigate failures |
| Performance regression | Benchmark before/after, monitor production metrics |
| Configuration complexity | Clear defaults, comprehensive documentation |

---

## Open Questions & Decisions

### âœ… Resolved

1. **Should RAG service be singleton?**
   - **Decision**: Yes, use singleton across all entry points
   - Need to verify if `RAGRetriever` has state/caching during implementation

2. **Retry/rate limiting in tests?**
   - **Decision**: Tests keep their own retry/rate limiting optimized for bulk testing
   - Not trying to unify these - different strategies serve different purposes
   - Tests validate LLM/RAG results, not infrastructure behavior

3. **Quote validation in tests?**
   - **Decision**: Add quote validation to quality tests (via orchestrator)
   - This is acceptable and desired for better test quality

4. **Multi-model context reuse?**
   - **Decision**: Orchestrator provides separate `retrieve_rag()` and `generate_with_context()` methods
   - Quality tests call `retrieve_rag()` once, then `generate_with_context()` N times
   - Preserves cost/performance optimization

5. **RAG pipeline features?**
   - **Decision**: Orchestrator delegates to RAG service with zero logic duplication
   - All features preserved: normalization, expansion, hybrid search, multi-hop, chunk limiting

### â“ Still Open

1. **Per-server config in tests?** How to specify model/config for test cases?
2. **Backwards compatibility?** Do we need to support old orchestration during migration?

---

## Timeline Estimate

- **Phase 1** (Create Orchestrator): 1-2 days
- **Phase 2** (Migrate Entry Points): 2-3 days
- **Phase 3** (Unified Retry): 1 day
- **Phase 4** (Configuration): 1 day
- **Testing & Validation**: 2-3 days
- **Documentation**: 1 day

**Total**: 8-11 days

---

## References

**Current Implementation Files**:
- [src/services/discord/bot.py:33-430](src/services/discord/bot.py) - `KillTeamBotOrchestrator`
- [src/cli/test_query.py:361-490](src/cli/test_query.py) - CLI orchestration
- [tests/quality/test_runner.py:46-484](tests/quality/test_runner.py) - Quality test orchestration
- [tests/rag/test_runner.py:34-150+](tests/rag/test_runner.py) - RAG test orchestration

**Related Services**:
- [src/services/rag/retriever.py](src/services/rag/retriever.py) - RAG service
- [src/services/llm/factory.py](src/services/llm/factory.py) - LLM provider factory
- [src/lib/constants.py](src/lib/constants.py) - Configuration constants

---

## RAG Feature Preservation Checklist

The orchestrator's `retrieve_rag()` method must preserve **all** existing RAG features by delegating to `RAGRetriever.retrieve()`:

### âœ… Feature Checklist

| Feature | Config Flag | Location | Status |
|---------|-------------|----------|--------|
| **Query Normalization** | `RAG_ENABLE_QUERY_NORMALIZATION` | `src/services/rag/retriever.py` | âœ… Preserved (delegation) |
| **Query Expansion** | `RAG_ENABLE_QUERY_EXPANSION` | `src/services/rag/retriever.py` | âœ… Preserved (delegation) |
| **Vector Search** | N/A (always enabled) | `src/services/rag/retriever.py` | âœ… Preserved (delegation) |
| **BM25 Search** | N/A (hybrid always enabled) | `src/services/rag/retriever.py` | âœ… Preserved (delegation) |
| **RRF Fusion** | N/A (hybrid always enabled) | `src/services/rag/retriever.py` | âœ… Preserved (delegation) |
| **Multi-Hop Retrieval** | Controlled by `max_hops` parameter | `src/services/rag/retriever.py` | âœ… Preserved (parameter passed) |
| **Team Filtering** | Part of multi-hop | `src/services/rag/retriever.py` | âœ… Preserved (multi-hop delegation) |
| **Hop Evaluation** | `RAG_HOP_EVALUATION_MODEL` | `src/services/rag/retriever.py` | âœ… Preserved (delegation) |
| **Final Chunk Limiting** | `MAXIMUM_FINAL_CHUNK_COUNT` | `src/services/rag/retriever.py` | âœ… Preserved (delegation) |

### Implementation Strategy

**Zero RAG logic in orchestrator** - all features implemented in `RAGRetriever`:

```python
# Orchestrator delegates completely
async def retrieve_rag(self, query: str, ...) -> tuple[RAGContext, ...]:
    # Just call RAG service - no logic here
    rag_context, hop_evals, _ = await self.rag.retrieve(
        RetrieveRequest(query=query, ...),
        query_id=uuid4(),
    )
    return rag_context, hop_evals, embedding_cost
```

**Verification**:
- Run RAG tests before and after migration
- Verify identical chunks retrieved for same query
- Verify multi-hop evaluation results unchanged
- Verify query normalization working (case-insensitive keywords)
- Verify chunk counts respect `MAXIMUM_FINAL_CHUNK_COUNT`
