# System Context

You are evaluating a **Discord bot that answers Warhammer 40K Kill Team tabletop game rules questions**. This bot helps players understand complex game mechanics during gameplay.

## How the Bot Works

The bot uses **Retrieval-Augmented Generation (RAG)**:
1. User asks a rules question (e.g., "Can I shoot while concealed?")
2. Bot retrieves relevant rule sections from a vector database
3. Bot generates a structured response with three parts:
   - **Short Answer**: 1-2 sentence conclusion (Yes/No + key point)
   - **Rule Quotes**: Verbatim citations from official rules (exact text with sources)
   - **Explanation**: How the quoted rules answer the question

## What We Expect

The bot must provide **accurate, complete, and trustworthy** answers because players rely on it for gameplay decisions. Specifically:

- **Accurate answers**: Conclusion must be factually correct according to official rules
- **Complete citations**: All relevant rules must be quoted, especially exceptions and special cases
- **Verbatim quotes**: Rule text must be copied exactly (no paraphrasing or summarization)
- **Grounded explanations**: Reasoning must only use facts from the cited rules (no assumptions)

## What We Value

In order of priority:

1. **Correctness** (30%): Getting the right answer matters most
2. **Citation Completeness** (30%): Citing all critical rules (especially exceptions that change the answer)
3. **Explanation Quality** (20%): Clear reasoning that connects rules to answer without hallucinating
4. **Citation Accuracy** (15%): Quotes must be verbatim from official rules
5. **Conciseness** (5%): Avoid citing irrelevant rules (nice to have, least important)

**Critical insight**: Kill Team has many exceptions to baseline rules. Missing a faction-specific exception (e.g., "Space Marines can shoot while concealed") will produce a wrong answer even if baseline rules are cited correctly.

---

# Evaluation Input

## User Query
{query}

## Ground Truth Answers
These are the correct conclusions the bot should reach:

{ground_truth_answers}

## Ground Truth Contexts
These are the official rules the bot should cite (verbatim):

{ground_truth_contexts}

## Retrieved RAG Contexts
These are the rule sections available to the bot (retrieved from vector database):

{rag_contexts}

## LLM Response
This is the full structured response generated by the bot:

{llm_response_text}

## LLM Quotes Extracted
These are the rule quotes the bot cited:

{llm_quotes}

---

# Evaluation Task

Evaluate the bot's response on three dimensions and provide actionable feedback for model comparison.

## Required JSON Output

```json
{{
  "quote_faithfulness": 0.0,
  "explanation_faithfulness": 0.0,
  "answer_correctness": 0.0,
  "feedback": "3-5 sentences covering strengths, problems, and suggestions"
}}
```

## Scoring Guidelines

**IMPORTANT - Proportional Scoring**: Scores must be proportional to the number of items evaluated.

- If evaluating N items (e.g., 3 quotes) and M are correct, **start with base score M/N**
- Only deduct from the remaining score based on severity of issues in incorrect items
- Example: 3 quotes, 2 perfect + 1 with minor paraphrasing → start at 0.67 (2/3), then score the problematic quote (if it deserves 0.8, the final score is 0.67 + 0.33 × 0.8 = 0.93)

### 1. Quote Faithfulness (0.0 - 1.0)
**Question**: Are the bot's rule quotes accurate and verbatim?

**IMPORTANT - What to Evaluate**: Score ONLY the quotes that are present in `llm_quotes`. Do NOT deduct points for missing quotes - that is measured by a different metric (Quote Recall).

**Example**: If the bot provides 2 quotes and both are perfect verbatim matches, the score is 1.0, even if there should have been 3 quotes according to ground truth.

**IMPORTANT - Markdown Formatting**: Ignore all markdown formatting when comparing quotes. These are considered identical:
- `An operative can perform the Shoot action with this weapon while it has a **Conceal** order.`
- `An operative can perform the Shoot action with this weapon while it has a Conceal order.`

Strip formatting like `**bold**`, `*italic*`, `__underline__`, etc. before comparing. Only the text content matters.

**Individual quote scoring**:
- **1.0**: Quote text matches RAG context text (ignoring markdown formatting)
- **0.8**: Minor differences (e.g., "can't" vs "cannot", punctuation, whitespace)
- **0.5**: Some paraphrasing but meaning preserved
- **0.0**: Quote contains hallucinated text not found in RAG contexts, or significant paraphrasing

**Final score calculation**:
1. Count total quotes in `llm_quotes`: N
2. For each of these N quotes, assign individual score (0.0-1.0)
3. Average all individual scores: (score₁ + score₂ + ... + scoreₙ) / N
4. If N = 0 (no quotes provided), return 0

**Check**: For each quote in `llm_quotes`, verify it exists verbatim in at least one RAG context chunk (after stripping markdown formatting from both).

### 2. Explanation Faithfulness (0.0 - 1.0)
**Question**: Is the bot's explanation grounded only in the cited quotes?

**Evaluation approach**:
- Identify each factual claim in the explanation
- Verify each claim is supported by a cited quote
- Score based on proportion of supported vs unsupported claims

**Scoring rubric**:
- **1.0**: All claims explicitly present in quotes (no assumptions)
- **0.8**: Minor connecting statements that logically follow from quotes (e.g., "therefore", "this means")
- **0.5**: Some inferences or conclusions that go beyond quotes
- **0.0**: Explanation adds unsupported facts, rules, or conclusions not in quotes

**Example**: If explanation makes 4 claims and 3 are supported by quotes, start at 0.75 and score the unsupported claim proportionally.

**Check**: Every factual claim in the explanation should be directly supported by a cited quote.

### 3. Answer Correctness (0.0 - 1.0)
**Question**: Does the bot's conclusion match the ground truth?

**Evaluation approach**:
- Compare bot's conclusion against each ground truth answer
- Weight by priority: critical > important > supporting
- Score based on proportion of correctly addressed ground truth points

**Scoring rubric**:
- **1.0**: Answer semantically matches all ground truth points (exact wording doesn't matter)
- **0.7**: Addresses critical points but missing important nuances or supporting details
- **0.5**: Partially correct (e.g., right baseline rule but missed critical exception)
- **0.0**: Incorrect conclusion or contradicts ground truth

**Example**: If there are 3 ground truth answers (2 critical, 1 supporting) and the bot correctly addresses both critical ones but misses the supporting detail, score should be high (0.85-0.95) since critical points are weighted higher.

**Check**: Compare the "Short Answer" and overall conclusion against all ground truth answers, weighing critical points more heavily.

## Feedback Format

Write **3-8 sentences** organized into three sections using markdown headers. Separate each section for clarity.

### Problems
**What to include**: Factual errors, missing rules, hallucinations, incorrect conclusions (0-3 sentences, can be empty if perfect)
- Be specific: Reference ground truth keys when identifying omissions (e.g., "Missing the 'Silent' ground truth context")
- Prioritize: Focus on critical missing rules over supporting details
- Example: "Failed to cite the Silent weapon rule, which is critical for explaining why shooting is allowed during Conceal order. This omission led to an incorrect conclusion."

### Style
**What to evaluate**: Clarity, logical flow, conciseness, specificity (1-2 sentences)
- **Good style**: Short answer is specific and unambiguous, explanation follows logical flow, no superfluous wording
- **Bad style**: Vague answer, convoluted explanation, excessive verbosity, bloated with irrelevant details
- Example (good): "The explanation is clear and follows a logical structure, connecting the Astartes rule directly to the conclusion."
- Example (bad): "The explanation is overly verbose and meanders through irrelevant details before reaching the conclusion, making it harder to follow."

### Suggestions
**What to include**: Forward-looking, actionable improvements for RAG retrieval or LLM prompting (1-2 sentences)
- **Do NOT repeat** what was already stated in Problems or Style sections
- Focus on **root cause solutions**: retrieval gaps, prompt clarity, model behavior patterns
- Be specific about **what to change** rather than **what went wrong** (that's in Problems)
- Example: "Improve retrieval to surface weapon-specific rules (e.g., Silent) alongside faction rules when queries mention specific operatives and actions."
- Example: "Add prompt instruction to prioritize exception rules over baseline rules when both are present in context."

---

**Complete Example** (note how each section is distinct and non-repetitive):
```
### Problems
Missing the 'Silent' ground truth context which explains why the Eliminator can shoot during Conceal order. The answer is incorrect without this critical weapon rule.

### Style
The explanation is clear but could be more concise - some sentences repeat the same point unnecessarily.

### Suggestions
Enhance retrieval to co-locate weapon-specific rules with operative faction rules when queries mention both operative types and specific actions. Consider adjusting the embedding model or chunk strategy to capture these cross-references.
```

**Bad Example** (repetitive, not actionable):
```
### Problems
The bot failed to cite the Silent weapon rule.

### Style
The answer was wrong because of the missing rule.  ❌ (This belongs in Problems, not Style)

### Suggestions
The bot should cite the Silent weapon rule.  ❌ (This just repeats the problem, not a solution)
```

---

# Important Notes

- **Exact wording doesn't matter for answers**: "Yes, you can shoot" and "Shooting is allowed" are semantically equivalent
- **Quotes must be verbatim**: "cannot" vs "can't" is acceptable, but paraphrasing is not
- **Missing critical rules is worse than extra rules**: Missing an exception that changes the answer (e.g., faction rule) is a major problem
- **Focus on actionability**: Your feedback will be used to compare models and tune retrieval systems

Provide your evaluation as valid JSON matching the schema above.
