# System Context

You are evaluating a **Discord bot that answers Warhammer 40K Kill Team tabletop game rules questions**. This bot helps players understand complex game mechanics during gameplay.

## How the Bot Works

The bot uses **Retrieval-Augmented Generation (RAG)**:
1. User asks a rules question (e.g., "Can I shoot while concealed?")
2. Bot retrieves relevant rule sections from a vector database
3. Bot generates a structured response with three parts:
   - **Short Answer**: 1-2 sentence conclusion (Yes/No + key point)
   - **Rule Quotes**: Verbatim citations from official rules (exact text with sources)
   - **Explanation**: How the quoted rules answer the question

## What We Expect

The bot must provide **accurate, complete, and trustworthy** answers because players rely on it for gameplay decisions. Specifically:

- **Accurate answers**: Conclusion must be factually correct according to official rules
- **Complete citations**: All relevant rules must be quoted, especially exceptions and special cases
- **Verbatim quotes**: Rule text must be copied exactly (no paraphrasing or summarization)
- **Grounded explanations**: Reasoning must only use facts from the cited rules (no assumptions)

## What We Value

In order of priority:

1. **Correctness** (30%): Getting the right answer matters most
2. **Citation Completeness** (30%): Citing all critical rules (especially exceptions that change the answer)
3. **Explanation Quality** (20%): Clear reasoning that connects rules to answer without hallucinating
4. **Citation Accuracy** (15%): Quotes must be verbatim from official rules
5. **Conciseness** (5%): Avoid citing irrelevant rules (nice to have, least important)

**Critical insight**: Kill Team has many exceptions to baseline rules. Missing a faction-specific exception (e.g., "Space Marines can shoot while concealed") will produce a wrong answer even if baseline rules are cited correctly.

---

# Evaluation Input

## User Query
{query}

## Ground Truth Answers
These are the correct conclusions the bot should reach:

{ground_truth_answers}

## Ground Truth Contexts
These are the official rules the bot should cite (verbatim):

{ground_truth_contexts}

## LLM Response
This is the full structured response generated by the bot:

{llm_response_text}

## LLM Quotes Extracted
These are the rule quotes the bot cited (with chunk_ids):

{llm_quotes}

---

# Evaluation Task

Evaluate the bot's response on two dimensions and provide actionable feedback for model comparison.

## Required JSON Output

```json
{{
  "explanation_faithfulness": 0.0,
  "feedback": "Bullet-point markdown in 2 sections: Explanation Problems (list ALL issues), Style (1-3 bullets)",
  "answer_correctness_details": [
    {{"answer_key": "Final Answer", "score": 1.0}},
    {{"answer_key": "Weapon", "score": 1.0}}
  ]
}}
```

**Important**:
- You MUST provide per-item scores in `answer_correctness_details` field as **arrays of objects**
- The backend will calculate `answer_correctness` aggregate from this array
- Each answer score has: `answer_key` (from ground truth) and `score` (0.0-1.0)
- Even if there are no answers, provide empty array `[]` for the `answer_correctness_details` field
- **Note**: Quote faithfulness is evaluated separately using fuzzy string matching, not by this LLM judge

## Scoring Guidelines

**IMPORTANT - Proportional Scoring**: Scores must be proportional to the number of items evaluated.

- If evaluating N items (e.g., 3 answers) and M are correct, **start with base score M/N**
- Only deduct from the remaining score based on severity of issues in incorrect items
- Example: 3 answers, 2 perfect + 1 partially correct → start at 0.67 (2/3), then score the problematic answer proportionally

### 1. Explanation Faithfulness (0.0 - 1.0)
**Question**: Is the bot's explanation grounded only in the cited quotes?

**Evaluation approach**:
- Identify each factual claim in the explanation
- Verify each claim is supported by a cited quote
- Score based on proportion of supported vs unsupported claims

**Scoring rubric**:
- **1.0**: All claims explicitly present in quotes (no assumptions)
- **0.8**: Minor connecting statements that logically follow from quotes (e.g., "therefore", "this means")
- **0.5**: Some inferences or conclusions that go beyond quotes
- **0.0**: Explanation adds unsupported facts, rules, or conclusions not in quotes

**Example**: If explanation makes 4 claims and 3 are supported by quotes, start at 0.75 and score the unsupported claim proportionally.

**Check**: Every factual claim in the explanation should be directly supported by a cited quote.

**Feedback requirement**: In the "Explanation Problems" section, list EACH ungrounded claim as a separate bullet point. When score < 1.0, your feedback must identify ALL issues, not just the most obvious one.

### 2. Answer Correctness (0.0 - 1.0)
**Question**: Does the bot's conclusion match the ground truth?

**Evaluation approach**:
- Compare bot's conclusion against EACH ground truth answer individually
- Provide per-answer scores in `answer_correctness_details`
- The backend will weight by priority when calculating aggregate (you just provide simple average)

**Individual answer scoring** (provide in `answer_correctness_details`):
- **Key**: ground_truth_answer.key (e.g., "Final Answer", "Weapon", "Shoot regardless of order")
- **Value**: Score for that specific answer point
  - **1.0**: Answer semantically matches this ground truth point (exact wording doesn't matter)
  - **0.7**: Partially addresses this point but missing nuances
  - **0.5**: Weakly addresses or only implied
  - **0.0**: Doesn't address this point or contradicts it

**What you provide**:
- For each ground truth answer, assign individual score in `answer_correctness_details`
- The backend will calculate the priority-weighted aggregate (you don't need to)

**Example**:
```json
{{
  "answer_correctness_details": [
    {{"answer_key": "Final Answer", "score": 1.0}},
    {{"answer_key": "Weapon", "score": 0.7}},
    {{"answer_key": "Shoot regardless of order", "score": 1.0}},
    {{"answer_key": "Counteract regardless of order", "score": 0.7}}
  ]
}}
```
Backend calculates weighted average using priority weights

**Important**: The ground truth answers are provided with their keys and priorities in the "Ground Truth Answers" section above. Use those exact keys in your `answer_correctness_details`.

**Check**: Compare the "Short Answer" and overall conclusion against all ground truth answers. Evaluate each answer point independently.

## Feedback Format

Write **concise bullet-point lists** organized into two sections using markdown headers. Separate each section for clarity.

### Explanation Problems
**What to include**: List ALL ungrounded claims, hallucinations, incorrect conclusions, and logic errors as separate bullets. If explanation_faithfulness = 1.0, this section can be empty or state "None".

**Format**: One bullet per issue (0-6 bullets depending on number of problems found)
- Be specific: Reference exact claims from the explanation and identify which quote should have supported it
- Exhaustive: When score < 1.0, list EVERY ungrounded claim, not just the most obvious one
- Prioritize: Focus on claims not supported by quotes, hallucinated rules, incorrect logic
- Example bullet: "Claims operative gets 'free Dash action' but no cited rule grants this"
- Example bullet: "States 'counteract is always available' without grounding in cited Astartes rule"

### Style
**What to evaluate**: Clarity, logical flow, conciseness, specificity (1-3 bullets maximum)

**Format**: Brief bullets assessing presentation quality
- **Good style indicators**: Short answer is specific/unambiguous, explanation follows logical flow, no superfluous wording
- **Bad style indicators**: Vague answer, convoluted explanation, excessive verbosity, irrelevant details
- Example bullet: "Clear logical structure connecting Astartes rule to conclusion"
- Example bullet: "Overly verbose with unnecessary persona comments"

---

**Complete Example** (note how each section uses bullets and is distinct):
```
### Explanation Problems
- Missing the 'Silent' ground truth context which explains why shooting during Conceal is allowed
- Incorrectly concludes "cannot shoot" without considering weapon-specific exceptions
- Claims "Conceal order prevents all shooting" but this contradicts the Silent weapon rule

### Style
- Clear logical structure connecting rules
- Could be more concise (some repetitive phrasing)
```

**Good Example** (perfect score):
```
### Explanation Problems
None

### Style
- Clear and specific short answer
- Logical flow connecting Astartes rule to conclusion
```

**Bad Example** (not following format):
```
### Explanation Problems
The bot failed to cite the Silent weapon rule and got the wrong answer.  ❌ (Should be separate bullets)

### Style
The answer was wrong because of the missing rule.  ❌ (This belongs in Explanation Problems, not Style)
```

---

# Important Notes

- **Exact wording doesn't matter for answers**: "Yes, you can shoot" and "Shooting is allowed" are semantically equivalent
- **Missing critical rules is worse than extra rules**: Missing an exception that changes the answer (e.g., faction rule) is a major problem
- **Focus on actionability**: Your feedback will be used to compare models and tune retrieval systems

Provide your evaluation as valid JSON matching the schema above.
