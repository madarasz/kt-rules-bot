# System Context

You are evaluating a **Discord bot that answers Warhammer 40K Kill Team tabletop game rules questions**. This bot helps players understand complex game mechanics during gameplay.

## How the Bot Works

The bot uses **Retrieval-Augmented Generation (RAG)**:
1. User asks a rules question (e.g., "Can I shoot while concealed?")
2. Bot retrieves relevant rule sections from a vector database
3. Bot generates a structured response with three parts:
   - **Short Answer**: 1-2 sentence conclusion (Yes/No + key point)
   - **Rule Quotes**: Verbatim citations from official rules (exact text with sources)
   - **Explanation**: How the quoted rules answer the question

## What We Expect

The bot must provide **accurate, complete, and trustworthy** answers because players rely on it for gameplay decisions. Specifically:

- **Accurate answers**: Conclusion must be factually correct according to official rules
- **Complete citations**: All relevant rules must be quoted, especially exceptions and special cases
- **Verbatim quotes**: Rule text must be copied exactly (no paraphrasing or summarization)
- **Grounded explanations**: Reasoning must only use facts from the cited rules (no assumptions)

## What We Value

In order of priority:

1. **Correctness** (30%): Getting the right answer matters most
2. **Citation Completeness** (30%): Citing all critical rules (especially exceptions that change the answer)
3. **Explanation Quality** (20%): Clear reasoning that connects rules to answer without hallucinating
4. **Citation Accuracy** (15%): Quotes must be verbatim from official rules
5. **Conciseness** (5%): Avoid citing irrelevant rules (nice to have, least important)

**Critical insight**: Kill Team has many exceptions to baseline rules. Missing a faction-specific exception (e.g., "Space Marines can shoot while concealed") will produce a wrong answer even if baseline rules are cited correctly.

---

# Evaluation Input

## User Query
{query}

## Ground Truth Answers
These are the correct conclusions the bot should reach:

{ground_truth_answers}

## Ground Truth Contexts
These are the official rules the bot should cite (verbatim):

{ground_truth_contexts}

## Retrieved RAG Contexts
These are the rule sections **referenced by the bot's quotes** (filtered by chunk_id for efficiency):

{rag_contexts}

**Note**: Only contexts referenced by quotes are shown. The bot had access to more contexts, but we filter here to focus evaluation.

## Quote to Context Mapping
Each quote's chunk_id maps to a specific RAG context above:

{quote_chunk_mapping}

## LLM Response
This is the full structured response generated by the bot:

{llm_response_text}

## LLM Quotes Extracted
These are the rule quotes the bot cited (with chunk_ids):

{llm_quotes}

---

# Evaluation Task

Evaluate the bot's response on three dimensions and provide actionable feedback for model comparison.

## Required JSON Output

```json
{{
  "explanation_faithfulness": 0.0,
  "feedback": "3-8 sentences in 3 sections: Problems, Style, Suggestions",
  "quote_faithfulness_details": [
    {{"chunk_id": "chunk_id_1", "score": 1.0}},
    {{"chunk_id": "chunk_id_2", "score": 0.8}}
  ],
  "answer_correctness_details": [
    {{"answer_key": "Final Answer", "score": 1.0}},
    {{"answer_key": "Weapon", "score": 1.0}}
  ]
}}
```

**Important**:
- You MUST provide per-item scores in `quote_faithfulness_details` and `answer_correctness_details` fields as **arrays of objects**
- The backend will calculate `quote_faithfulness` and `answer_correctness` aggregates from these arrays
- Each quote score has: `chunk_id` (last 8 chars of UUID) and `score` (0.0-1.0)
- Each answer score has: `answer_key` (from ground truth) and `score` (0.0-1.0)
- Even if there are no quotes or answers, provide empty arrays `[]` for the `*_details` fields

## Scoring Guidelines

**IMPORTANT - Proportional Scoring**: Scores must be proportional to the number of items evaluated.

- If evaluating N items (e.g., 3 quotes) and M are correct, **start with base score M/N**
- Only deduct from the remaining score based on severity of issues in incorrect items
- Example: 3 quotes, 2 perfect + 1 with minor paraphrasing → start at 0.67 (2/3), then score the problematic quote (if it deserves 0.8, the final score is 0.67 + 0.33 × 0.8 = 0.93)

### 1. Quote Faithfulness (0.0 - 1.0)
**Question**: Are the bot's rule quotes accurate and verbatim?

**IMPORTANT - What to Evaluate**: Score ONLY the quotes that are present in `llm_quotes`. Do NOT deduct points for missing quotes - that is measured by a different metric (Quote Recall).

**Example**: If the bot provides 2 quotes and both are perfect verbatim matches, the score is 1.0, even if there should have been 3 quotes according to ground truth.

**IMPORTANT - chunk_id Format**: The chunk_id is the **last 8 characters** of the chunk's UUID (e.g., "a1b2c3d4"). Use this to match quotes to RAG contexts.

**Individual quote scoring** (provide in `quote_faithfulness_details`):
- **Key**: chunk_id (last 8 chars, e.g., "a1b2c3d4")
- **Value**: Score for that specific quote
  - **1.0**: Quote text matches RAG context text verbatim
  - **0.8**: Minor differences (e.g., "can't" vs "cannot", punctuation, whitespace)
  - **0.5**: Some paraphrasing but meaning preserved
  - **0.0**: Quote contains hallucinated text not found in RAG contexts, or significant paraphrasing

**What you provide**:
- For each quote, assign individual score in `quote_faithfulness_details`
- The backend will calculate the aggregate (you don't need to)

**Example**:
```json
{{
  "quote_faithfulness_details": [
    {{"chunk_id": "a1b2c3d4", "score": 1.0}},
    {{"chunk_id": "b2c3d4e5", "score": 0.8}},
    {{"chunk_id": "c3d4e5f6", "score": 0.9}}
  ]
}}
```
Backend calculates: (1.0 + 0.8 + 0.9) / 3 = 0.9

**Check**: For each quote in `llm_quotes`, verify it exists verbatim in the RAG context chunk matching its chunk_id.

### 2. Explanation Faithfulness (0.0 - 1.0)
**Question**: Is the bot's explanation grounded only in the cited quotes?

**Evaluation approach**:
- Identify each factual claim in the explanation
- Verify each claim is supported by a cited quote
- Score based on proportion of supported vs unsupported claims

**Scoring rubric**:
- **1.0**: All claims explicitly present in quotes (no assumptions)
- **0.8**: Minor connecting statements that logically follow from quotes (e.g., "therefore", "this means")
- **0.5**: Some inferences or conclusions that go beyond quotes
- **0.0**: Explanation adds unsupported facts, rules, or conclusions not in quotes

**Example**: If explanation makes 4 claims and 3 are supported by quotes, start at 0.75 and score the unsupported claim proportionally.

**Check**: Every factual claim in the explanation should be directly supported by a cited quote.

### 3. Answer Correctness (0.0 - 1.0)
**Question**: Does the bot's conclusion match the ground truth?

**Evaluation approach**:
- Compare bot's conclusion against EACH ground truth answer individually
- Provide per-answer scores in `answer_correctness_details`
- The backend will weight by priority when calculating aggregate (you just provide simple average)

**Individual answer scoring** (provide in `answer_correctness_details`):
- **Key**: ground_truth_answer.key (e.g., "Final Answer", "Weapon", "Shoot regardless of order")
- **Value**: Score for that specific answer point
  - **1.0**: Answer semantically matches this ground truth point (exact wording doesn't matter)
  - **0.7**: Partially addresses this point but missing nuances
  - **0.5**: Weakly addresses or only implied
  - **0.0**: Doesn't address this point or contradicts it

**What you provide**:
- For each ground truth answer, assign individual score in `answer_correctness_details`
- The backend will calculate the priority-weighted aggregate (you don't need to)

**Example**:
```json
{{
  "answer_correctness_details": [
    {{"answer_key": "Final Answer", "score": 1.0}},
    {{"answer_key": "Weapon", "score": 0.7}},
    {{"answer_key": "Shoot regardless of order", "score": 1.0}},
    {{"answer_key": "Counteract regardless of order", "score": 0.7}}
  ]
}}
```
Backend calculates weighted average using priority weights

**Important**: The ground truth answers are provided with their keys and priorities in the "Ground Truth Answers" section above. Use those exact keys in your `answer_correctness_details`.

**Check**: Compare the "Short Answer" and overall conclusion against all ground truth answers. Evaluate each answer point independently.

## Feedback Format

Write **3-8 sentences** organized into three sections using markdown headers. Separate each section for clarity.

### Problems
**What to include**: Factual errors, missing rules, hallucinations, incorrect conclusions (0-3 sentences, can be empty if perfect)
- Be specific: Reference ground truth keys when identifying omissions (e.g., "Missing the 'Silent' ground truth context")
- Prioritize: Focus on critical missing rules over supporting details
- Example: "Failed to cite the Silent weapon rule, which is critical for explaining why shooting is allowed during Conceal order. This omission led to an incorrect conclusion."

### Style
**What to evaluate**: Clarity, logical flow, conciseness, specificity (1-2 sentences)
- **Good style**: Short answer is specific and unambiguous, explanation follows logical flow, no superfluous wording
- **Bad style**: Vague answer, convoluted explanation, excessive verbosity, bloated with irrelevant details
- Example (good): "The explanation is clear and follows a logical structure, connecting the Astartes rule directly to the conclusion."
- Example (bad): "The explanation is overly verbose and meanders through irrelevant details before reaching the conclusion, making it harder to follow."

### Suggestions
**What to include**: Forward-looking, actionable improvements for RAG retrieval or LLM prompting (1-2 sentences)
- **Do NOT repeat** what was already stated in Problems or Style sections
- Focus on **root cause solutions**: retrieval gaps, prompt clarity, model behavior patterns
- Be specific about **what to change** rather than **what went wrong** (that's in Problems)
- Example: "Improve retrieval to surface weapon-specific rules (e.g., Silent) alongside faction rules when queries mention specific operatives and actions."
- Example: "Add prompt instruction to prioritize exception rules over baseline rules when both are present in context."

---

**Complete Example** (note how each section is distinct and non-repetitive):
```
### Problems
Missing the 'Silent' ground truth context which explains why the Eliminator can shoot during Conceal order. The answer is incorrect without this critical weapon rule.

### Style
The explanation is clear but could be more concise - some sentences repeat the same point unnecessarily.

### Suggestions
Enhance retrieval to co-locate weapon-specific rules with operative faction rules when queries mention both operative types and specific actions. Consider adjusting the embedding model or chunk strategy to capture these cross-references.
```

**Bad Example** (repetitive, not actionable):
```
### Problems
The bot failed to cite the Silent weapon rule.

### Style
The answer was wrong because of the missing rule.  ❌ (This belongs in Problems, not Style)

### Suggestions
The bot should cite the Silent weapon rule.  ❌ (This just repeats the problem, not a solution)
```

---

# Important Notes

- **Exact wording doesn't matter for answers**: "Yes, you can shoot" and "Shooting is allowed" are semantically equivalent
- **Quotes must be verbatim**: "cannot" vs "can't" is acceptable, but paraphrasing is not
- **Missing critical rules is worse than extra rules**: Missing an exception that changes the answer (e.g., faction rule) is a major problem
- **Focus on actionability**: Your feedback will be used to compare models and tune retrieval systems

Provide your evaluation as valid JSON matching the schema above.
